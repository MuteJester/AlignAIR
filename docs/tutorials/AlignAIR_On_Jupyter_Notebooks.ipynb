{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1e19cc",
   "metadata": {},
   "source": [
    "## How to Train and Predict with AlignAIR v2.0 Models in a Jupyter Environment\n",
    "\n",
    "This notebook demonstrates how to train and predict with AlignAIR's unified v2.0 architecture in a Jupyter environment. We will cover the following steps:\n",
    "\n",
    "1. **Training the Model**: We will train a SingleChainAlignAIR or MultiChainAlignAIR model using a sample dataset.\n",
    "2. **Saving the Model**: After training, we will save the model weights for future use.\n",
    "3. **Loading Pretrained Model Weights**: We will load the saved model weights.\n",
    "4. **Using the Loaded Model**: Finally, we will use the loaded model to make predictions on new sequences.\n",
    "\n",
    "### What's New in v2.0\n",
    "- **Unified Architecture**: `SingleChainAlignAIR` and `MultiChainAlignAIR` replace chain-specific models\n",
    "- **Dynamic GenAIRR Integration**: Built-in dataconfigs for major receptor types\n",
    "- **Multi-Chain Support**: Native support for mixed receptor analysis\n",
    "- **Streamlined API**: Simplified training and prediction workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Import v2.0 unified architecture components\n",
    "from AlignAIR.Models.SingleChainAlignAIR import SingleChainAlignAIR\n",
    "from AlignAIR.Models.MultiChainAlignAIR import MultiChainAlignAIR\n",
    "from AlignAIR.Data.SingleChainDataset import SingleChainDataset\n",
    "from AlignAIR.Data.MultiChainDataset import MultiChainDataset\n",
    "from AlignAIR.Data.MultiDataConfigContainer import MultiDataConfigContainer\n",
    "from AlignAIR.Trainers import Trainer\n",
    "\n",
    "# GenAIRR integration for dynamic data configuration\n",
    "from GenAIRR.data import (\n",
    "    builtin_heavy_chain_data_config,\n",
    "    builtin_kappa_chain_data_config,\n",
    "    builtin_lambda_chain_data_config\n",
    ")\n",
    "\n",
    "# Legacy imports for backward compatibility\n",
    "from AlignAIR.Metadata import RandomDataConfigGenerator\n",
    "from AlignAIR.PostProcessing.HeuristicMatching import HeuristicReferenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e92700",
   "metadata": {},
   "source": [
    "# Training The Model\n",
    "\n",
    "In this section, we will train a SingleChainAlignAIR or MultiChainAlignAIR model using the unified v2.0 architecture. The training process involves the following steps:\n",
    "\n",
    "1. **Dataset Preparation**: Load and prepare the training dataset using the new unified dataset classes.\n",
    "2. **Model Selection**: Choose between SingleChainAlignAIR (optimized for single receptor type) or MultiChainAlignAIR (supports multiple receptor types).\n",
    "3. **Training**: Train the model using the prepared dataset.\n",
    "4. **Saving the Model**: Save the trained model weights for future use.\n",
    "\n",
    "### Model Selection Guide\n",
    "- **SingleChainAlignAIR**: Use when training on a single receptor type (e.g., only IGH sequences)\n",
    "- **MultiChainAlignAIR**: Use when training on mixed receptor types (e.g., IGH + IGK + IGL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb932a",
   "metadata": {},
   "source": [
    "### Dataset Requirements\n",
    "Before loading the training dataset, ensure it contains the following columns:\n",
    "\n",
    "- **sequence**: The nucleotide sequence.\n",
    "- **v_sequence_start**: Start position of the V gene segment.\n",
    "- **v_sequence_end**: End position of the V gene segment.\n",
    "- **d_sequence_start**: Start position of the D gene segment.\n",
    "- **d_sequence_end**: End position of the D gene segment.\n",
    "- **j_sequence_start**: Start position of the J gene segment.\n",
    "- **j_sequence_end**: End position of the J gene segment.\n",
    "- **v_call**: V gene call.\n",
    "- **d_call**: D gene call.\n",
    "- **j_call**: J gene call.\n",
    "- **mutation_rate**: Mutation rate in the sequence.\n",
    "- **indels**: Insertions and deletions in the sequence.\n",
    "- **productive**: Whether the sequence is productive or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d663fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequence',\n",
       " 'v_sequence_start',\n",
       " 'v_sequence_end',\n",
       " 'd_sequence_start',\n",
       " 'd_sequence_end',\n",
       " 'j_sequence_start',\n",
       " 'j_sequence_end',\n",
       " 'v_call',\n",
       " 'd_call',\n",
       " 'j_call',\n",
       " 'mutation_rate',\n",
       " 'indels',\n",
       " 'productive']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are the required columns your training dataset\n",
    "['sequence', 'v_sequence_start', 'v_sequence_end', 'd_sequence_start',\n",
    "                                      'd_sequence_end', 'j_sequence_start', 'j_sequence_end', 'v_call',\n",
    "                                      'd_call', 'j_call', 'mutation_rate', 'indels', 'productive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e00291",
   "metadata": {},
   "source": [
    "## Loading the Training Dataset\n",
    "\n",
    "### Option 1: Single-Chain Training (SingleChainDataset)\n",
    "\n",
    "To train on a single receptor type, use the `SingleChainDataset` class:\n",
    "\n",
    "1. **Specify the Dataset Path**: Ensure you have the correct path to your dataset file (TSV, CSV, or FASTA format).\n",
    "2. **Create Data Configuration**: Use built-in GenAIRR dataconfigs or load custom ones.\n",
    "3. **Instantiate SingleChainDataset**: Create an instance with the dataset path and data configuration.\n",
    "\n",
    "### Option 2: Multi-Chain Training (MultiChainDataset)\n",
    "\n",
    "To train on multiple receptor types, use the `MultiChainDataset` class with `MultiDataConfigContainer`:\n",
    "\n",
    "1. **Create MultiDataConfigContainer**: Combine multiple GenAIRR dataconfigs for different chain types.\n",
    "2. **Instantiate MultiChainDataset**: The dataset will automatically handle mixed receptor types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Single-Chain Training Dataset (e.g., Heavy Chain only)\n",
    "dataset_path = '/path/to/your/dataset.csv'  # replace with your path, can be tsv, csv or fasta\n",
    "dataconfig_instance = builtin_heavy_chain_data_config()  # or builtin_kappa_chain_data_config(), builtin_lambda_chain_data_config()\n",
    "\n",
    "# For single-chain training\n",
    "train_dataset = SingleChainDataset(\n",
    "    data_path=dataset_path,\n",
    "    dataconfig=dataconfig_instance,\n",
    "    use_streaming=True,\n",
    "    max_sequence_length=576\n",
    ")\n",
    "\n",
    "# Example 2: Multi-Chain Training Dataset (mixed receptor types)\n",
    "# Create a multi-dataconfig container for multiple chain types\n",
    "multi_dataconfig = MultiDataConfigContainer({\n",
    "    'IGH': builtin_heavy_chain_data_config(),\n",
    "    'IGK': builtin_kappa_chain_data_config(), \n",
    "    'IGL': builtin_lambda_chain_data_config()\n",
    "})\n",
    "\n",
    "# For multi-chain training (when dataset contains mixed receptor types)\n",
    "multi_train_dataset = MultiChainDataset(\n",
    "    data_path=dataset_path,\n",
    "    multi_dataconfig=multi_dataconfig,\n",
    "    use_streaming=True,\n",
    "    max_sequence_length=576\n",
    ")\n",
    "\n",
    "# Use train_dataset for SingleChainAlignAIR or multi_train_dataset for MultiChainAlignAIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec78c3",
   "metadata": {},
   "source": [
    "## Setting Up the Trainer\n",
    "\n",
    "In this section, we will set up the `Trainer` class to train our unified AlignAIR v2.0 model. Follow these steps:\n",
    "\n",
    "1. **Choose Model Architecture**: Select between `SingleChainAlignAIR` or `MultiChainAlignAIR` based on your dataset.\n",
    "\n",
    "2. **Initialize Trainer**: Create an instance of the `Trainer` class with the following parameters:\n",
    "   - `model`: Either `SingleChainAlignAIR` (for single receptor type) or `MultiChainAlignAIR` (for multi-chain analysis)\n",
    "   - `dataset`: The corresponding dataset object (`SingleChainDataset` or `MultiChainDataset`)\n",
    "   - `epochs`: Number of epochs (e.g., 1)\n",
    "   - `steps_per_epoch`: Number of steps per epoch (e.g., 512)\n",
    "   - `verbose`: Verbosity level (e.g., 1 for detailed logging)\n",
    "   - `classification_metric`: List of AUC metrics (only used for logging)\n",
    "   - `regression_metric`: Binary cross-entropy loss (only used for logging)\n",
    "   - `optimizers_params`: Dictionary with optimizer parameters (e.g., gradient clipping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda23b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Single-Chain Model Training\n",
    "trainer_single = Trainer(\n",
    "    model=SingleChainAlignAIR,\n",
    "    dataset=train_dataset,  # SingleChainDataset instance\n",
    "    epochs=1,\n",
    "    steps_per_epoch=max(1, train_dataset.data_length // 10),\n",
    "    verbose=1,\n",
    "    classification_metric=[tf.keras.metrics.AUC(), tf.keras.metrics.AUC(), tf.keras.metrics.AUC()],\n",
    "    regression_metric=tf.keras.losses.binary_crossentropy,\n",
    "    optimizers_params={\"clipnorm\": 1},\n",
    ")\n",
    "\n",
    "# Option 2: Multi-Chain Model Training\n",
    "trainer_multi = Trainer(\n",
    "    model=MultiChainAlignAIR,\n",
    "    dataset=multi_train_dataset,  # MultiChainDataset instance  \n",
    "    epochs=1,\n",
    "    steps_per_epoch=max(1, multi_train_dataset.data_length // 10),\n",
    "    verbose=1,\n",
    "    classification_metric=[tf.keras.metrics.AUC(), tf.keras.metrics.AUC(), tf.keras.metrics.AUC()],\n",
    "    regression_metric=tf.keras.losses.binary_crossentropy,\n",
    "    optimizers_params={\"clipnorm\": 1},\n",
    ")\n",
    "\n",
    "# Choose the appropriate trainer based on your use case\n",
    "trainer = trainer_single  # or trainer_multi for multi-chain analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31443b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - d_allele_auc_1: 0.5000 - d_end_binary_crossentropy: 602.0290 - d_start_binary_crossentropy: 5207.7612 - j_allele_auc_2: 0.5000 - j_end_binary_crossentropy: 641.9519 - j_start_binary_crossentropy: 1340.1698 - v_allele_auc: 0.5000 - v_end_binary_crossentropy: 3360.4792 - v_start_binary_crossentropy: 314.3087 - loss: 2188.4346 - scaled_classification_loss: 2.5794 - scaled_indel_count_loss: 1.3750 - scaled_productivity_loss: 0.7311 - scaled_mutation_rate_loss: 0.1316 - segmentation_loss: 2183.6177 - average_last_label: 0.5000 - v_allele_entropy: 68.6216 - d_allele_entropy: 11.7835 - j_allele_entropy: 2.4260 - classification_loss: 2.5794 - mutation_rate_loss: 0.1316 - indel_count_loss: 1.3750 - productive_loss: 0.7311\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffadd340",
   "metadata": {},
   "source": [
    "### Saving Your Trained Model Weights for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a34f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_weights(f'your/path/model_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca645fc",
   "metadata": {},
   "source": [
    "## Configuring the Model for Inference\n",
    "\n",
    "In this section, we will set up the `Trainer` class to configure our unified AlignAIR v2.0 model for inference. Follow these steps:\n",
    "\n",
    "1. **Ensure Consistency**: Make sure the dataset object has the same `DataConfig` or `MultiDataConfigContainer` as during training to ensure consistency. Use a small dataset sample (e.g., 10 samples) for configuration.\n",
    "\n",
    "2. **Initialize Trainer**: Create an instance of the `Trainer` class matching your training setup:\n",
    "   - For single-chain: Use `SingleChainAlignAIR` with `SingleChainDataset`\n",
    "   - For multi-chain: Use `MultiChainAlignAIR` with `MultiChainDataset`\n",
    "\n",
    "3. **Build the Model**: Use the `build` method to define the model architecture with the input shape (e.g., tokenized sequence of shape (576, 1)).\n",
    "\n",
    "4. **Load Model Weights**: Load the pre-trained model weights from the specified checkpoint path (`MODEL_CHECKPOINT`).\n",
    "\n",
    "This setup prepares the `Trainer` for configuring the model for inference with the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad167e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure trainer for inference - ensure same setup as training\n",
    "# Option 1: Single-Chain Model Inference\n",
    "trainer_inference = Trainer(\n",
    "    model=SingleChainAlignAIR,  # Match your training model\n",
    "    dataset=train_dataset,  # Same dataconfig as training, small sample for configuration\n",
    "    epochs=1,\n",
    "    steps_per_epoch=512,\n",
    "    verbose=1,\n",
    "    classification_metric=[tf.keras.metrics.AUC(), tf.keras.metrics.AUC(), tf.keras.metrics.AUC()],\n",
    "    regression_metric=tf.keras.losses.binary_crossentropy,\n",
    "    optimizers_params={\"clipnorm\": 1},\n",
    ")\n",
    "\n",
    "# Option 2: Multi-Chain Model Inference (if you trained with MultiChainAlignAIR)\n",
    "# trainer_inference = Trainer(\n",
    "#     model=MultiChainAlignAIR,\n",
    "#     dataset=multi_train_dataset,  # Same multi_dataconfig as training\n",
    "#     epochs=1,\n",
    "#     steps_per_epoch=512,\n",
    "#     verbose=1,\n",
    "#     classification_metric=[tf.keras.metrics.AUC(), tf.keras.metrics.AUC(), tf.keras.metrics.AUC()],\n",
    "#     regression_metric=tf.keras.losses.binary_crossentropy,\n",
    "#     optimizers_params={\"clipnorm\": 1},\n",
    "# )\n",
    "\n",
    "# Build the model so trained weights can be mounted\n",
    "trainer_inference.model.build({'tokenized_sequence': (576, 1)})\n",
    "MODEL_CHECKPOINT = 'your/path/model_name'\n",
    "trainer_inference.model.load_weights(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7cb7d7",
   "metadata": {},
   "source": [
    "## Running the Prediction Pipeline\n",
    "\n",
    "In this section, we will set up and run a prediction pipeline using the `AlignAIR` library. This pipeline processes input data, makes predictions, and performs various post-processing steps to generate final results. Follow these steps:\n",
    "\n",
    "1. **Import Necessary Modules**: Import the required modules and classes from the `AlignAIR` library for preprocessing, model loading, batch processing, and post-processing tasks.\n",
    "\n",
    "2. **Create Logger**: Create a logger named `PipelineLogger` to log the process. Logging helps in tracking progress and debugging issues.\n",
    "\n",
    "3. **Instantiate PredictObject**: Create an instance of the `PredictObject` class with the necessary arguments and the logger. This object will hold all the predicted information and processed results throughout the pipeline.\n",
    "\n",
    "4. **Define Pipeline Steps**: Define the pipeline as a list of steps, each represented by an instance of a specific class from the `AlignAIR` library. These steps include:\n",
    "   - Loading configuration\n",
    "   - Extracting file names\n",
    "   - Counting samples\n",
    "   - Loading models\n",
    "   - Processing and predicting batches\n",
    "   - Cleaning up raw predictions\n",
    "   - Correcting segmentations\n",
    "   - Applying thresholds to distill assignments\n",
    "   - Aligning predicted segments with germline sequences\n",
    "   - Translating alleles to IMGT format\n",
    "   - Finalizing post-processing and saving results as a CSV file\n",
    "\n",
    "5. **Execute Pipeline**: Run the pipeline by executing each step sequentially. The `execute` method of each step processes the `predict_object` and updates it with the results of that step. This ensures that the data flows through all necessary stages to produce the final output.\n",
    "\n",
    "By following these steps, you will be able to set up and run the prediction pipeline to generate the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3105b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\"\"\"\n",
    "Here we load all the parameters needed for using the complete AlignAIR v2.0 suite, including the post-processing and pre-processing steps. \n",
    "This is usually done via CLI, thus we imitate the parameters one would pass in the command line and load all of them into an argparse namespace.\n",
    "\n",
    "Note: In v2.0, --chain-type has been replaced with --genairr-dataconfig for dynamic GenAIRR integration.\n",
    "\"\"\"\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    mode=None,\n",
    "    config_file='',  # this is for the YAML file mode and is not relevant here\n",
    "    model_checkpoint=r'C:\\Users\\tomas\\Desktop\\AlignAIRR\\tests\\AlignAIRR_S5F_OGRDB_V8_S5F_576_Balanced_V2',  # checkpoint of trained model weights\n",
    "    save_path='/Users/tomas/Downloads/',  # path for the saved results\n",
    "    genairr_dataconfig='HUMAN_IGH_OGRDB',  # NEW v2.0: GenAIRR dataconfig (replaces chain_type)\n",
    "    sequences=r'C:\\Users\\tomas\\Desktop\\AlignAIRR\\tests\\sample_HeavyChain_dataset.csv',  # target sequences (csv/tsv/FASTA)\n",
    "    max_input_size=576,  # max input size, must match the trained model\n",
    "    batch_size=8,  # maximum number of samples per batch processed by the model\n",
    "    v_allele_threshold=0.1,  # threshold for v allele call likelihood consideration\n",
    "    d_allele_threshold=0.1,  # threshold for d allele call likelihood consideration\n",
    "    j_allele_threshold=0.1,  # threshold for j allele call likelihood consideration\n",
    "    v_cap=3,  # maximum number of v allele calls based on likelihood and threshold\n",
    "    d_cap=3,  # maximum number of d allele calls based on likelihood and threshold\n",
    "    j_cap=3,  # maximum number of j allele calls based on likelihood and threshold\n",
    "    translate_to_asc=False,  # translate ASC's to IMGT allele names if ASC were derived\n",
    "    fix_orientation=True,  # check and orient reversed sequences properly\n",
    "    custom_orientation_pipeline_path=None  # path to custom orientation pipeline if needed\n",
    ")\n",
    "\n",
    "# Available GenAIRR dataconfigs in v2.0:\n",
    "# - 'HUMAN_IGH_OGRDB' (Heavy chain)\n",
    "# - 'HUMAN_IGK_OGRDB' (Kappa light chain)  \n",
    "# - 'HUMAN_IGL_OGRDB' (Lambda light chain)\n",
    "# - 'HUMAN_TCRB_IMGT' (TCR Beta chain)\n",
    "# - Custom dataconfig path for your own species/references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aea94ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecde1326c76646c7a88e0f333a22d35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing V Likelihoods:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf5d80e32f644d3b3cea7483e7cb58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing J Likelihoods:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adc0ff056eb46359527eb15c7829c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing D Likelihoods:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75f6b99cad54eae910ca4b7b5bfc804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching V Germlines:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f788d6054ed4ff8a1e11da249c8f2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching J Germlines:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0163d5d1218d4bb58c679db2fa3e141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching D Germlines:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from AlignAIR.PostProcessing.Steps.allele_threshold_step import MaxLikelihoodPercentageThresholdApplicationStep, \\\n",
    "    ConfidenceMethodThresholdApplicationStep\n",
    "from AlignAIR.PostProcessing.Steps.clean_up_steps import CleanAndArrangeStep\n",
    "from AlignAIR.PostProcessing.Steps.finalization_and_packaging_steps import FinalizationStep\n",
    "from AlignAIR.PostProcessing.Steps.germline_alignment_steps import AlleleAlignmentStep\n",
    "from AlignAIR.PostProcessing.Steps.segmentation_correction_steps import SegmentCorrectionStep\n",
    "from AlignAIR.PostProcessing.Steps.translate_to_imgt_step import TranslationStep\n",
    "from AlignAIR.PredictObject.PredictObject import PredictObject\n",
    "from AlignAIR.Preprocessing.Steps.batch_processing_steps import BatchProcessingStep\n",
    "from AlignAIR.Preprocessing.Steps.dataconfig_steps import ConfigLoadStep\n",
    "from AlignAIR.Preprocessing.Steps.file_steps import FileNameExtractionStep, FileSampleCounterStep\n",
    "from AlignAIR.Preprocessing.Steps.model_loading_steps import ModelLoadingStep\n",
    "import logging\n",
    "from AlignAIR.Step.Step import Step\n",
    "\n",
    "# create a logger to log the process\n",
    "logger = logging.getLogger('PipelineLogger')\n",
    "Step.set_logger(logger)\n",
    "\n",
    "# set up t he predict objecet, here all the predicted information and processed results will be saved\n",
    "predict_object = PredictObject(args, logger=logger)\n",
    "\n",
    "# define the steps in the prediction pipeline\n",
    "steps = [\n",
    "    ConfigLoadStep(\"Load Config\"),\n",
    "    FileNameExtractionStep('Get File Name'),\n",
    "    FileSampleCounterStep('Count Samples in File'),\n",
    "    ModelLoadingStep('Load Models'),\n",
    "    BatchProcessingStep(\"Process and Predict Batches\"),\n",
    "    CleanAndArrangeStep(\"Clean Up Raw Prediction\"),\n",
    "    SegmentCorrectionStep(\"Correct Segmentations\"),\n",
    "    MaxLikelihoodPercentageThresholdApplicationStep(\"Apply Max Likelihood Threshold to Distill Assignments\"),\n",
    "    AlleleAlignmentStep(\"Align Predicted Segments with Germline\"),\n",
    "    TranslationStep(\"Translate ASC's to IMGT Alleles\"),\n",
    "    FinalizationStep(\"Finalize Post Processing and Save Csv\")\n",
    "]\n",
    "\n",
    "#run the pipeline\n",
    "for step in steps:\n",
    "    predict_object = step.execute(predict_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw prediction made by the model before any processing can be found here:\n",
    "predict_object.results['predictions']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlignAIR_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
