{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1e19cc",
   "metadata": {},
   "source": [
    "## How to Train and Predict with an AlignAIR Model in a Jupyter Enviorment\n",
    "Introduction\n",
    "\n",
    "This notebook demonstrates how to train and predict with an AlignAIR model in a Jupyter environment. We will cover the following steps:\n",
    "\n",
    "1. **Training the Model**: We will train a HeavyChain AlignAIR model using a sample dataset.\n",
    "2. **Saving the Model**: After training, we will save the model weights for future use.\n",
    "3. **Loading Pretrained Model Weights**: We will load the saved model weights.\n",
    "4. **Using the Loaded Model**: Finally, we will use the loaded model to make predictions on new sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from AlignAIR.Metadata import RandomDataConfigGenerator\n",
    "from AlignAIR.Models.LightChain import LightChainAlignAIRR\n",
    "import pandas as pd\n",
    "from GenAIRR.data import builtin_heavy_chain_data_config,builtin_kappa_chain_data_config,builtin_lambda_chain_data_config\n",
    "from AlignAIR.Data import HeavyChainDataset, LightChainDataset\n",
    "from AlignAIR.Models.HeavyChain import HeavyChainAlignAIRR\n",
    "from AlignAIR.Trainers import Trainer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from AlignAIR.PostProcessing.HeuristicMatching import HeuristicReferenceMatcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e92700",
   "metadata": {},
   "source": [
    "# Training The Model\n",
    "\n",
    "In this section, we will train a HeavyChain AlignAIR model using a sample dataset. The training process involves the following steps:\n",
    "\n",
    "1. **Dataset Preparation**: Load and prepare the training dataset.\n",
    "2. **Model Initialization**: Define and initialize the model.\n",
    "3. **Training**: Train the model using the prepared dataset.\n",
    "4. **Saving the Model**: Save the trained model weights for future use.\n",
    "\n",
    "We will start by going over an example of how a HeavyChain AlignAIR is trained. The LightChain training process is essentially the same with slight differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb932a",
   "metadata": {},
   "source": [
    "### Dataset Requirements\n",
    "Before loading the training dataset, ensure it contains the following columns:\n",
    "\n",
    "- **sequence**: The nucleotide sequence.\n",
    "- **v_sequence_start**: Start position of the V gene segment.\n",
    "- **v_sequence_end**: End position of the V gene segment.\n",
    "- **d_sequence_start**: Start position of the D gene segment.\n",
    "- **d_sequence_end**: End position of the D gene segment.\n",
    "- **j_sequence_start**: Start position of the J gene segment.\n",
    "- **j_sequence_end**: End position of the J gene segment.\n",
    "- **v_call**: V gene call.\n",
    "- **d_call**: D gene call.\n",
    "- **j_call**: J gene call.\n",
    "- **mutation_rate**: Mutation rate in the sequence.\n",
    "- **indels**: Insertions and deletions in the sequence.\n",
    "- **productive**: Whether the sequence is productive or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d663fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequence',\n",
       " 'v_sequence_start',\n",
       " 'v_sequence_end',\n",
       " 'd_sequence_start',\n",
       " 'd_sequence_end',\n",
       " 'j_sequence_start',\n",
       " 'j_sequence_end',\n",
       " 'v_call',\n",
       " 'd_call',\n",
       " 'j_call',\n",
       " 'mutation_rate',\n",
       " 'indels',\n",
       " 'productive']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are the required columns your training dataset\n",
    "['sequence', 'v_sequence_start', 'v_sequence_end', 'd_sequence_start',\n",
    "                                      'd_sequence_end', 'j_sequence_start', 'j_sequence_end', 'v_call',\n",
    "                                      'd_call', 'j_call', 'mutation_rate', 'indels', 'productive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e00291",
   "metadata": {},
   "source": [
    "## Loading the Training Dataset\n",
    "\n",
    "To begin, we need to load our training dataset into the `HeavyChainDataset` class. Follow these steps:\n",
    "\n",
    "1. **Specify the Dataset Path**: Ensure you have the correct path to your dataset file (TSV, CSV, or FASTA format). Replace `heavy_chain_dataset_path` with the actual file path.\n",
    "\n",
    "2. **Create Data Configuration**: Use the `builtin_heavy_chain_data_config()` function to load the builtin BCR HeavyChain configuration instance. This ensures that the alleles in your dataset match the reference alleles.\n",
    "(You can always load the pickled custom DataConfig instance you create using GenAIR for your own data/species/reference)\n",
    "\n",
    "3. **Instantiate HeavyChainDataset**: Create an instance of the `HeavyChainDataset` class with the dataset path and data configuration. Set `batch_read_file` to `True` for efficient handling of large datasets and define the `max_sequence_length` (e.g., 576).\n",
    "\n",
    "This setup prepares your dataset for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Your Training Dataset into a HeavyChainDataset Instance\n",
    "heavy_chain_dataset_path = '/path/to/your/dataset.csv' # replace with your path, can be tsv,csv or fasta\n",
    "dataconfig_insatnce = builtin_heavy_chain_data_config() # make sure the dataconfig you are using matches your dataset (the alleles in your dataset should share the same reference for the V,D and J alleles as the dataconfig object)\n",
    "train_dataset = HeavyChainDataset(data_path=heavy_chain_dataset_path,\n",
    "                                          dataconfig=dataconfig_insatnce ,batch_read_file=True,\n",
    "                                          max_sequence_length=576)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec78c3",
   "metadata": {},
   "source": [
    "## Setting Up the Trainer\n",
    "\n",
    "In this section, we will set up the `Trainer` class to train our `HeavyChainAlignAIRR` model. Follow these steps:\n",
    "\n",
    "1. **Ensure Consistency**: Make sure the `train_dataset` object has the same `DataConfig` as during training to ensure consistency.\n",
    "\n",
    "2. **Initialize Trainer**: Create an instance of the `Trainer` class with the following parameters:\n",
    "   - `model`: The `HeavyChainAlignAIRR` model. (can be replaced with LightChainAlignAIRR or any future version of the AlignAIR)\n",
    "   - `dataset`: The `train_dataset` object such as HeavyChainDataset or LightChain Dataset for example.\n",
    "   - `epochs`: Number of epochs (e.g., 1).\n",
    "   - `steps_per_epoch`: Number of steps per epoch (e.g., 512).\n",
    "   - `verbose`: Verbosity level (e.g., 1 for detailed logging).\n",
    "   - `classification_metric`: List of AUC metrics. (only used for logging)\n",
    "   - `regression_metric`: Binary cross-entropy loss. (only used for logging)\n",
    "   - `optimizers_params`: Dictionary with optimizer parameters (e.g., gradient clipping).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda23b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh\n"
     ]
    }
   ],
   "source": [
    "# define a Trainer instance which will handle the initialization and training process of the model\n",
    "trainer = Trainer(\n",
    "    model=HeavyChainAlignAIRR,\n",
    "    dataset=train_dataset,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=max(1, train_dataset.data_length // 10),\n",
    "    verbose=1,\n",
    "    classification_metric=[tf.keras.metrics.AUC(), tf.keras.metrics.AUC(), tf.keras.metrics.AUC()],\n",
    "    regression_metric=tf.keras.losses.binary_crossentropy,\n",
    "    optimizers_params={\"clipnorm\": 1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31443b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - d_allele_auc_1: 0.5000 - d_end_binary_crossentropy: 602.0290 - d_start_binary_crossentropy: 5207.7612 - j_allele_auc_2: 0.5000 - j_end_binary_crossentropy: 641.9519 - j_start_binary_crossentropy: 1340.1698 - v_allele_auc: 0.5000 - v_end_binary_crossentropy: 3360.4792 - v_start_binary_crossentropy: 314.3087 - loss: 2188.4346 - scaled_classification_loss: 2.5794 - scaled_indel_count_loss: 1.3750 - scaled_productivity_loss: 0.7311 - scaled_mutation_rate_loss: 0.1316 - segmentation_loss: 2183.6177 - average_last_label: 0.5000 - v_allele_entropy: 68.6216 - d_allele_entropy: 11.7835 - j_allele_entropy: 2.4260 - classification_loss: 2.5794 - mutation_rate_loss: 0.1316 - indel_count_loss: 1.3750 - productive_loss: 0.7311\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffadd340",
   "metadata": {},
   "source": [
    "### Saving Your Trained Model Weights for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a34f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_weights(f'your/path/model_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca645fc",
   "metadata": {},
   "source": [
    "## Configuring the Trainer for Inference\n",
    "\n",
    "In this section, we will set up the `Trainer` class to configure our `HeavyChainAlignAIRR` model for inference. Follow these steps:\n",
    "\n",
    "1. **Ensure Consistency**: Make sure the `train_dataset` object has the same `DataConfig` as during training to ensure consistency. Use a small dataset sample (e.g., 10 samples) for configuration.\n",
    "\n",
    "2. **Initialize Trainer**: Create an instance of the `Trainer` class with the following parameters:\n",
    "   - `model`: The `HeavyChainAlignAIRR` model.\n",
    "   - `dataset`: The `train_dataset` object.\n",
    "   - `epochs`: Number of epochs (e.g., 1).\n",
    "   - `steps_per_epoch`: Number of steps per epoch (e.g., 512).\n",
    "   - `verbose`: Verbosity level (e.g., 1 for detailed logging).\n",
    "   - `classification_metric`: List of AUC metrics.\n",
    "   - `regression_metric`: Binary cross-entropy loss.\n",
    "   - `optimizers_params`: Dictionary with optimizer parameters (e.g., gradient clipping).\n",
    "\n",
    "3. **Build the Model**: Use the `build` method to define the model architecture with the input shape (e.g., tokenized sequence of shape (576, 1)).\n",
    "\n",
    "4. **Load Model Weights**: Load the pre-trained model weights from the specified checkpoint path (`MODEL_CHECKPOINT`).\n",
    "\n",
    "This setup prepares the `Trainer` for configuring the model for inference with the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad167e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the trainer is defiend the same way it was defined in the training in terms of the train_dataset provided to it (the train_dataset object must have the same DataConfig asscotiated to it)\n",
    "# it will not be used for inference only to define the model, my suggestion is to use a the same dataconfig object as in training and a small dataset sample of 10 samples just for configuring the model\n",
    "trainer = Trainer(\n",
    "    model=HeavyChainAlignAIRR,\n",
    "    dataset=train_dataset,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=512,\n",
    "    verbose=1,\n",
    "    classification_metric=[tf.keras.metrics.AUC(), tf.keras.metrics.AUC(), tf.keras.metrics.AUC()],\n",
    "    regression_metric=tf.keras.losses.binary_crossentropy,\n",
    "    optimizers_params={\"clipnorm\": 1},\n",
    ")\n",
    "\n",
    "# build the model so trained the weights can be mounted \n",
    "trainer.model.build({'tokenized_sequence': (576, 1)})\n",
    "MODEL_CHECKPOINT = f'your/path/model_name'\n",
    "trainer.model.load_weights(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7cb7d7",
   "metadata": {},
   "source": [
    "## Running the Prediction Pipeline\n",
    "\n",
    "In this section, we will set up and run a prediction pipeline using the `AlignAIR` library. This pipeline processes input data, makes predictions, and performs various post-processing steps to generate final results. Follow these steps:\n",
    "\n",
    "1. **Import Necessary Modules**: Import the required modules and classes from the `AlignAIR` library for preprocessing, model loading, batch processing, and post-processing tasks.\n",
    "\n",
    "2. **Create Logger**: Create a logger named `PipelineLogger` to log the process. Logging helps in tracking progress and debugging issues.\n",
    "\n",
    "3. **Instantiate PredictObject**: Create an instance of the `PredictObject` class with the necessary arguments and the logger. This object will hold all the predicted information and processed results throughout the pipeline.\n",
    "\n",
    "4. **Define Pipeline Steps**: Define the pipeline as a list of steps, each represented by an instance of a specific class from the `AlignAIR` library. These steps include:\n",
    "   - Loading configuration\n",
    "   - Extracting file names\n",
    "   - Counting samples\n",
    "   - Loading models\n",
    "   - Processing and predicting batches\n",
    "   - Cleaning up raw predictions\n",
    "   - Correcting segmentations\n",
    "   - Applying thresholds to distill assignments\n",
    "   - Aligning predicted segments with germline sequences\n",
    "   - Translating alleles to IMGT format\n",
    "   - Finalizing post-processing and saving results as a CSV file\n",
    "\n",
    "5. **Execute Pipeline**: Run the pipeline by executing each step sequentially. The `execute` method of each step processes the `predict_object` and updates it with the results of that step. This ensures that the data flows through all necessary stages to produce the final output.\n",
    "\n",
    "By following these steps, you will be able to set up and run the prediction pipeline to generate the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3105b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\"\"\"\n",
    "Here we load all the parameters needed for using the complete AlignAIR suite, including the post-processing and pre-processing steps. \n",
    "This is usually done via Docker or CLI, thus we imitate the parameters one would pass in the command line and load all of them into an argparse namespace.\n",
    "\"\"\"\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    mode=None,\n",
    "    config_file='',# this is for the YAML file mode and is not relevant here,\n",
    "    model_checkpoint=r'C:\\Users\\tomas\\Desktop\\AlignAIRR\\tests\\AlignAIRR_S5F_OGRDB_V8_S5F_576_Balanced_V2', # check point of trained model weights\n",
    "    save_path='/Users/tomas/Downloads/', # path for the saved results\n",
    "    chain_type='heavy', # type of chain i.e heavy/light\n",
    "    sequences=r'C:\\Users\\tomas\\Desktop\\AlignAIRR\\tests\\sample_HeavyChain_dataset.csv', # the target sequences, can be csv/tsv/FASTA file, csv and tsv must have a column called \"sequecne\"\n",
    "    lambda_data_config='D', # if custom lambda dataconfig is required else leave as \"D\"\n",
    "    kappa_data_config='D', # if custom kappa dataconfig is required else leave as \"D\"\n",
    "    heavy_data_config='D', # if custom heavy chain dataconfig is required else leave as \"D\"\n",
    "    max_input_size=576, # max input size, has to match the max_size of the trained model\n",
    "    batch_size=8, # the maximum number of samples per batch processed by the model\n",
    "    v_allele_threshold=0.1, # the threshold for v allele call likelihood consideration\n",
    "    d_allele_threshold=0.1, # the threshold for d allele call likelihood consideration\n",
    "    j_allele_threshold=0.1, # the threshold for j allele call likelihood consideration\n",
    "    v_cap=3, # the maximum number of v allele calls the model will select based on the likelihood predicted and the threshold\n",
    "    d_cap=3, # the maximum number of d allele calls the model will select based on the likelihood predicted and the threshold\n",
    "    j_cap=3, # the maximum number of j allele calls the model will select based on the likelihood predicted and the threshold\n",
    "    translate_to_asc=False, # in case ASC were derived for the DataConfig, this will transalte the ASC's to IMGT allele names\n",
    "    fix_orientation=True, # this flag controls wheter the preprocessing should check if there are reversed sequences and orient them properly\n",
    "    custom_orientation_pipeline_path=None # in case you have a custom model you will need to create a custom orientation pipeline, and specify the path here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aea94ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecde1326c76646c7a88e0f333a22d35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing V Likelihoods:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf5d80e32f644d3b3cea7483e7cb58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing J Likelihoods:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adc0ff056eb46359527eb15c7829c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing D Likelihoods:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75f6b99cad54eae910ca4b7b5bfc804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching V Germlines:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f788d6054ed4ff8a1e11da249c8f2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching J Germlines:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0163d5d1218d4bb58c679db2fa3e141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Matching D Germlines:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from AlignAIR.PostProcessing.Steps.allele_threshold_step import MaxLikelihoodPercentageThresholdApplicationStep, \\\n",
    "    ConfidenceMethodThresholdApplicationStep\n",
    "from AlignAIR.PostProcessing.Steps.clean_up_steps import CleanAndArrangeStep\n",
    "from AlignAIR.PostProcessing.Steps.finalization_and_packaging_steps import FinalizationStep\n",
    "from AlignAIR.PostProcessing.Steps.germline_alignment_steps import AlleleAlignmentStep\n",
    "from AlignAIR.PostProcessing.Steps.segmentation_correction_steps import SegmentCorrectionStep\n",
    "from AlignAIR.PostProcessing.Steps.translate_to_imgt_step import TranslationStep\n",
    "from AlignAIR.PredictObject.PredictObject import PredictObject\n",
    "from AlignAIR.Preprocessing.Steps.batch_processing_steps import BatchProcessingStep\n",
    "from AlignAIR.Preprocessing.Steps.dataconfig_steps import ConfigLoadStep\n",
    "from AlignAIR.Preprocessing.Steps.file_steps import FileNameExtractionStep, FileSampleCounterStep\n",
    "from AlignAIR.Preprocessing.Steps.model_loading_steps import ModelLoadingStep\n",
    "import logging\n",
    "\n",
    "# create a logger to log the process\n",
    "logger = logging.getLogger('PipelineLogger')\n",
    "# set up t he predict objecet, here all the predicted information and processed results will be saved\n",
    "predict_object = PredictObject(args, logger=logger)\n",
    "\n",
    "# define the steps in the prediction pipeline\n",
    "steps = [\n",
    "    ConfigLoadStep(\"Load Config\", logger),\n",
    "    FileNameExtractionStep('Get File Name', logger),\n",
    "    FileSampleCounterStep('Count Samples in File', logger),\n",
    "    ModelLoadingStep('Load Models', logger),\n",
    "    BatchProcessingStep(\"Process and Predict Batches\", logger),\n",
    "    CleanAndArrangeStep(\"Clean Up Raw Prediction\", logger),\n",
    "    SegmentCorrectionStep(\"Correct Segmentations\", logger),\n",
    "    MaxLikelihoodPercentageThresholdApplicationStep(\"Apply Max Likelihood Threshold to Distill Assignments\", logger),\n",
    "    AlleleAlignmentStep(\"Align Predicted Segments with Germline\", logger),\n",
    "    TranslationStep(\"Translate ASC's to IMGT Alleles\", logger),\n",
    "    FinalizationStep(\"Finalize Post Processing and Save Csv\", logger)\n",
    "]\n",
    "\n",
    "#run the pipeline\n",
    "for step in steps:\n",
    "    predict_object = step.execute(predict_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw prediction made by the model before any processing can be found here:\n",
    "predict_object.results['predictions']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlignAIR_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
